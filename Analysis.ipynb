{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1bcea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593b6767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.10.2+cu102\n",
      "torchvision version: 0.11.3+cu102\n",
      "pytorch lightning version: 1.5.1\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import signal\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "print('torch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "print('pytorch lightning version:', pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db00009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=4096, checkpoint_dir=PosixPath('checkpoint'), data=PosixPath(' '), epochs=1000, lambd=0.0051, learning_rate_biases=0.0048, learning_rate_weights=0.2, print_freq=100, projector='1024-1024-1024', weight_decay=1e-06, workers=8)\n"
     ]
    }
   ],
   "source": [
    "    parser = argparse.ArgumentParser(description='Barlow Twins Training')\n",
    "    parser.add_argument('data', type=Path, metavar='DIR',\n",
    "                        help='path to dataset')\n",
    "    parser.add_argument('--workers', default=8, type=int, metavar='N',\n",
    "                        help='number of data loader workers')\n",
    "    parser.add_argument('--epochs', default=1000, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('--batch-size', default=4096, type=int, metavar='N',\n",
    "                        help='mini-batch size')\n",
    "    parser.add_argument('--learning-rate-weights', default=0.2, type=float, metavar='LR',\n",
    "                        help='base learning rate for weights')\n",
    "    parser.add_argument('--learning-rate-biases', default=0.0048, type=float, metavar='LR',\n",
    "                        help='base learning rate for biases and batch norm parameters')\n",
    "    parser.add_argument('--weight-decay', default=1e-6, type=float, metavar='W',\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--lambd', default=0.0051, type=float, metavar='L',\n",
    "                        help='weight on off-diagonal terms')\n",
    "    # parser.add_argument('--projector', default='8192-8192-8192', type=str,\n",
    "    #                     metavar='MLP', help='projector MLP')\n",
    "    parser.add_argument('--projector', default='1024-1024-1024', type=str,\n",
    "                        metavar='MLP', help='projector MLP')\n",
    "    parser.add_argument('--print-freq', default=100, type=int, metavar='N',\n",
    "                        help='print frequency')\n",
    "    parser.add_argument('--checkpoint-dir', default='./checkpoint/', type=Path,\n",
    "                        metavar='DIR', help='path to checkpoint directory')\n",
    "\n",
    "    args = parser.parse_args(\" \") \n",
    "    print(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96a1afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86cf9867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "ds_train length: 50000\n",
      "Files already downloaded and verified\n",
      "ds_val length: 10000\n"
     ]
    }
   ],
   "source": [
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "from BarlowTwins_CIFAR10_classifier_dataset import *\n",
    "\n",
    "geo_transforms = A.Compose(\n",
    "    [\n",
    "#      A.HorizontalFlip(p=0.5),\n",
    "#      A.VerticalFlip(p=0.5),\n",
    "     A.Normalize(),\n",
    "     ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pixel_transforms = A.Compose(\n",
    "    [\n",
    "     ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 200\n",
    "ds_train = BT_CIFAR10_Classify_Dataset(train=True, geo_transform=geo_transforms, pixel_transform=pixel_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "print('ds_train length:', ds_train.__len__())\n",
    "\n",
    "ds_val = BT_CIFAR10_Classify_Dataset(train=False, geo_transform=geo_transforms, pixel_transform=pixel_transforms)\n",
    "valloader = torch.utils.data.DataLoader(ds_val, batch_size=batch_size, shuffle=False, num_workers=2) \n",
    "print('ds_val length:', ds_val.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "292bb5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 200\n",
    "\n",
    "# ds_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# ds_val = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "# valloader = torch.utils.data.DataLoader(ds_val, batch_size=batch_size, shuffle=False, num_workers=2)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8772dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 4096\n",
      "Creating new base encoder\n"
     ]
    }
   ],
   "source": [
    "from resnet50_cifar10_classifier import *\n",
    "#model = resnet50_cifar10_classifier(ds_train, ds_val, args, base_barlow_model_encoder=None)\n",
    "model = resnet50_cifar10_classifier.load_from_checkpoint(checkpoint_path='./lightning_logs/barlow_classifier/version_1/checkpoints/epoch=50-step=662.ckpt', \n",
    "                                   ds_train=ds_train, ds_val=ds_val, args=args, base_barlow_model_encoder=None)\n",
    "# model = torch.load('./lightning_logs/barlow_classifier/version_1/checkpoints/epoch=66-step=870.ckpt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f5bab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "batch: 10\n",
      "batch: 20\n",
      "batch: 30\n",
      "batch: 40\n",
      "accuracy: tensor(0.5442)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# initialize metric\n",
    "accuracy = Accuracy(num_classes=10, threshold=0.5, top_k=1)\n",
    "\n",
    "for (i, batch) in enumerate(valloader):\n",
    "    x, target = batch\n",
    "    #target = (torch.nn.functional.one_hot(target, num_classes=10))\n",
    "\n",
    "    preds = model.forward(x)\n",
    "    #preds = (torch.argmax(preds, dim=-1)).type(torch.int64) #.unsqueeze(-1)).type(torch.int64)    \n",
    "    \n",
    "    # metric on current batch\n",
    "    accuracy.update(preds, target)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('batch:', i)\n",
    "    \n",
    "\n",
    "# metric on all batches using custom accumulation\n",
    "print('accuracy:', accuracy.compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756bafeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1024, checkpoint_dir=PosixPath('checkpoint'), data=PosixPath(' '), epochs=1000, lambd=0.0051, learning_rate_biases=0.0048, learning_rate_weights=0.2, print_freq=100, projector='8192-8192-8192', weight_decay=1e-06, workers=8)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Barlow Twins Training')\n",
    "parser.add_argument('data', type=Path, metavar='DIR',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--workers', default=8, type=int, metavar='N',\n",
    "                    help='number of data loader workers')\n",
    "parser.add_argument('--epochs', default=1000, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--batch-size', default=1024, type=int, metavar='N',\n",
    "                    help='mini-batch size')\n",
    "parser.add_argument('--learning-rate-weights', default=0.2, type=float, metavar='LR',\n",
    "                    help='base learning rate for weights')\n",
    "parser.add_argument('--learning-rate-biases', default=0.0048, type=float, metavar='LR',\n",
    "                    help='base learning rate for biases and batch norm parameters')\n",
    "parser.add_argument('--weight-decay', default=1e-6, type=float, metavar='W',\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--lambd', default=0.0051, type=float, metavar='L',\n",
    "                    help='weight on off-diagonal terms')\n",
    "parser.add_argument('--projector', default='8192-8192-8192', type=str,\n",
    "                    metavar='MLP', help='projector MLP')\n",
    "parser.add_argument('--print-freq', default=100, type=int, metavar='N',\n",
    "                    help='print frequency')\n",
    "parser.add_argument('--checkpoint-dir', default='./checkpoint/', type=Path,\n",
    "                    metavar='DIR', help='path to checkpoint directory')\n",
    "\n",
    "args = parser.parse_args(\" \") \n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd342b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from barlowtwins_resnet50 import *\n",
    "\n",
    "# Load previously trained barlowtwins_resnet50 model\n",
    "# Expects to have base barlow model initialized from checkpoint\n",
    "bt_resnet50_chkpt = './lightning_logs/barlow/version_1/checkpoints/epoch=58-step=2890.ckpt'\n",
    "blah = barlowtwins_resnet50.load_from_checkpoint(checkpoint_path=bt_resnet50_chkpt, ds_train=None, args=args)\n",
    "blah.freeze()\n",
    "encoder = blah.encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad61e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d17cddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable params in model: 0\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, blah.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Number of trainable params in model:', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d48b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ac0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7640d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c8111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
